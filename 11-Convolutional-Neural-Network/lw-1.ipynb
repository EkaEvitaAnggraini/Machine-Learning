{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "In this practical exercise, we will create a simple CNN classification model for the case of cat and dog images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pre Processing\n",
    "\n",
    "In this stage, we will perform some image manipulations on the images used. The manipulations include pixel value normalization, tilt correction, enlargement (zoom), and flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1. Training Data Pre Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\test_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.2. Testing Data Pre Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Build CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.1. - CNN Model Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.2. - Add 1st Convolutional Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.3. - Add 1st Pooling Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.4. - Add 2nd Convolutional Layer and Pooling Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.5. - Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.6. - Add Fully Connected Layer (Input and 1st Hidden Layer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.7. Add Fully Connected Layer (The Output Layer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.8. - Compile CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 14s 219ms/step - loss: 0.6820 - accuracy: 0.5615 - val_loss: 0.6379 - val_accuracy: 0.6535\n",
      "Epoch 2/60\n",
      "63/63 [==============================] - 12s 197ms/step - loss: 0.6401 - accuracy: 0.6435 - val_loss: 0.5849 - val_accuracy: 0.6980\n",
      "Epoch 3/60\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.6037 - accuracy: 0.6805 - val_loss: 0.5527 - val_accuracy: 0.7295\n",
      "Epoch 4/60\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.5872 - accuracy: 0.6925 - val_loss: 0.5819 - val_accuracy: 0.6910\n",
      "Epoch 5/60\n",
      "63/63 [==============================] - 16s 250ms/step - loss: 0.5736 - accuracy: 0.7020 - val_loss: 0.5629 - val_accuracy: 0.7245\n",
      "Epoch 6/60\n",
      "63/63 [==============================] - 17s 278ms/step - loss: 0.5534 - accuracy: 0.7170 - val_loss: 0.5292 - val_accuracy: 0.7335\n",
      "Epoch 7/60\n",
      "63/63 [==============================] - 18s 286ms/step - loss: 0.5408 - accuracy: 0.7245 - val_loss: 0.4840 - val_accuracy: 0.7600\n",
      "Epoch 8/60\n",
      "63/63 [==============================] - 18s 284ms/step - loss: 0.5291 - accuracy: 0.7350 - val_loss: 0.5001 - val_accuracy: 0.7490\n",
      "Epoch 9/60\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.5043 - accuracy: 0.7600 - val_loss: 0.4775 - val_accuracy: 0.7615\n",
      "Epoch 10/60\n",
      "63/63 [==============================] - 17s 264ms/step - loss: 0.4892 - accuracy: 0.7695 - val_loss: 0.4358 - val_accuracy: 0.8045\n",
      "Epoch 11/60\n",
      "63/63 [==============================] - 17s 274ms/step - loss: 0.5024 - accuracy: 0.7530 - val_loss: 0.4704 - val_accuracy: 0.7640\n",
      "Epoch 12/60\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.5031 - accuracy: 0.7645 - val_loss: 0.4196 - val_accuracy: 0.8105\n",
      "Epoch 13/60\n",
      "63/63 [==============================] - 15s 243ms/step - loss: 0.4550 - accuracy: 0.7835 - val_loss: 0.4085 - val_accuracy: 0.8075\n",
      "Epoch 14/60\n",
      "63/63 [==============================] - 13s 203ms/step - loss: 0.4337 - accuracy: 0.7970 - val_loss: 0.3929 - val_accuracy: 0.8265\n",
      "Epoch 15/60\n",
      "63/63 [==============================] - 13s 201ms/step - loss: 0.4159 - accuracy: 0.8085 - val_loss: 0.3423 - val_accuracy: 0.8585\n",
      "Epoch 16/60\n",
      "63/63 [==============================] - 14s 216ms/step - loss: 0.4153 - accuracy: 0.8085 - val_loss: 0.3384 - val_accuracy: 0.8640\n",
      "Epoch 17/60\n",
      "63/63 [==============================] - 16s 254ms/step - loss: 0.4011 - accuracy: 0.8235 - val_loss: 0.3157 - val_accuracy: 0.8755\n",
      "Epoch 18/60\n",
      "63/63 [==============================] - 15s 230ms/step - loss: 0.3787 - accuracy: 0.8220 - val_loss: 0.3317 - val_accuracy: 0.8500\n",
      "Epoch 19/60\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.3747 - accuracy: 0.8310 - val_loss: 0.3179 - val_accuracy: 0.8610\n",
      "Epoch 20/60\n",
      "63/63 [==============================] - 16s 259ms/step - loss: 0.3688 - accuracy: 0.8455 - val_loss: 0.2719 - val_accuracy: 0.8925\n",
      "Epoch 21/60\n",
      "63/63 [==============================] - 13s 203ms/step - loss: 0.3423 - accuracy: 0.8485 - val_loss: 0.2526 - val_accuracy: 0.8990\n",
      "Epoch 22/60\n",
      "63/63 [==============================] - 16s 249ms/step - loss: 0.3408 - accuracy: 0.8540 - val_loss: 0.2439 - val_accuracy: 0.8950\n",
      "Epoch 23/60\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.3103 - accuracy: 0.8680 - val_loss: 0.2130 - val_accuracy: 0.9220\n",
      "Epoch 24/60\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.2950 - accuracy: 0.8725 - val_loss: 0.2058 - val_accuracy: 0.9285\n",
      "Epoch 25/60\n",
      "63/63 [==============================] - 21s 326ms/step - loss: 0.2816 - accuracy: 0.8800 - val_loss: 0.1970 - val_accuracy: 0.9240\n",
      "Epoch 26/60\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.2829 - accuracy: 0.8820 - val_loss: 0.1825 - val_accuracy: 0.9295\n",
      "Epoch 27/60\n",
      "63/63 [==============================] - 16s 248ms/step - loss: 0.2680 - accuracy: 0.8860 - val_loss: 0.1700 - val_accuracy: 0.9410\n",
      "Epoch 28/60\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 0.2508 - accuracy: 0.8915 - val_loss: 0.1639 - val_accuracy: 0.9340\n",
      "Epoch 29/60\n",
      "63/63 [==============================] - 15s 230ms/step - loss: 0.2426 - accuracy: 0.8990 - val_loss: 0.1786 - val_accuracy: 0.9245\n",
      "Epoch 30/60\n",
      "63/63 [==============================] - 13s 209ms/step - loss: 0.2101 - accuracy: 0.9120 - val_loss: 0.1302 - val_accuracy: 0.9575\n",
      "Epoch 31/60\n",
      "63/63 [==============================] - 14s 221ms/step - loss: 0.1938 - accuracy: 0.9255 - val_loss: 0.1658 - val_accuracy: 0.9345\n",
      "Epoch 32/60\n",
      "63/63 [==============================] - 14s 225ms/step - loss: 0.2092 - accuracy: 0.9160 - val_loss: 0.1158 - val_accuracy: 0.9620\n",
      "Epoch 33/60\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.1661 - accuracy: 0.9435 - val_loss: 0.1123 - val_accuracy: 0.9580\n",
      "Epoch 34/60\n",
      "63/63 [==============================] - 14s 226ms/step - loss: 0.1854 - accuracy: 0.9195 - val_loss: 0.1157 - val_accuracy: 0.9595\n",
      "Epoch 35/60\n",
      "63/63 [==============================] - 14s 228ms/step - loss: 0.1607 - accuracy: 0.9385 - val_loss: 0.0700 - val_accuracy: 0.9830\n",
      "Epoch 36/60\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 0.1457 - accuracy: 0.9440 - val_loss: 0.0832 - val_accuracy: 0.9700\n",
      "Epoch 37/60\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.1436 - accuracy: 0.9420 - val_loss: 0.0803 - val_accuracy: 0.9735\n",
      "Epoch 38/60\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.1423 - accuracy: 0.9440 - val_loss: 0.0625 - val_accuracy: 0.9790\n",
      "Epoch 39/60\n",
      "63/63 [==============================] - 13s 211ms/step - loss: 0.1315 - accuracy: 0.9535 - val_loss: 0.0664 - val_accuracy: 0.9835\n",
      "Epoch 40/60\n",
      "63/63 [==============================] - 14s 224ms/step - loss: 0.1246 - accuracy: 0.9540 - val_loss: 0.0543 - val_accuracy: 0.9830\n",
      "Epoch 41/60\n",
      "63/63 [==============================] - 15s 243ms/step - loss: 0.1006 - accuracy: 0.9700 - val_loss: 0.0664 - val_accuracy: 0.9715\n",
      "Epoch 42/60\n",
      "63/63 [==============================] - 15s 236ms/step - loss: 0.1278 - accuracy: 0.9530 - val_loss: 0.0520 - val_accuracy: 0.9865\n",
      "Epoch 43/60\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.1136 - accuracy: 0.9525 - val_loss: 0.0565 - val_accuracy: 0.9825\n",
      "Epoch 44/60\n",
      "63/63 [==============================] - 15s 232ms/step - loss: 0.1006 - accuracy: 0.9620 - val_loss: 0.0370 - val_accuracy: 0.9885\n",
      "Epoch 45/60\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.0941 - accuracy: 0.9690 - val_loss: 0.0373 - val_accuracy: 0.9855\n",
      "Epoch 46/60\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 0.0874 - accuracy: 0.9710 - val_loss: 0.0469 - val_accuracy: 0.9875\n",
      "Epoch 47/60\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.0897 - accuracy: 0.9715 - val_loss: 0.0518 - val_accuracy: 0.9855\n",
      "Epoch 48/60\n",
      "63/63 [==============================] - 13s 215ms/step - loss: 0.0678 - accuracy: 0.9755 - val_loss: 0.0512 - val_accuracy: 0.9815\n",
      "Epoch 49/60\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 0.0858 - accuracy: 0.9690 - val_loss: 0.0267 - val_accuracy: 0.9945\n",
      "Epoch 50/60\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.1040 - accuracy: 0.9550 - val_loss: 0.0470 - val_accuracy: 0.9850\n",
      "Epoch 51/60\n",
      "63/63 [==============================] - 15s 236ms/step - loss: 0.0765 - accuracy: 0.9740 - val_loss: 0.0230 - val_accuracy: 0.9955\n",
      "Epoch 52/60\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.0726 - accuracy: 0.9750 - val_loss: 0.0453 - val_accuracy: 0.9850\n",
      "Epoch 53/60\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.0797 - accuracy: 0.9720 - val_loss: 0.0264 - val_accuracy: 0.9945\n",
      "Epoch 54/60\n",
      "63/63 [==============================] - 15s 234ms/step - loss: 0.0702 - accuracy: 0.9745 - val_loss: 0.0274 - val_accuracy: 0.9940\n",
      "Epoch 55/60\n",
      "63/63 [==============================] - 13s 210ms/step - loss: 0.0641 - accuracy: 0.9805 - val_loss: 0.0314 - val_accuracy: 0.9910\n",
      "Epoch 56/60\n",
      "63/63 [==============================] - 20s 312ms/step - loss: 0.0492 - accuracy: 0.9850 - val_loss: 0.0311 - val_accuracy: 0.9915\n",
      "Epoch 57/60\n",
      "63/63 [==============================] - 16s 260ms/step - loss: 0.0615 - accuracy: 0.9800 - val_loss: 0.0870 - val_accuracy: 0.9680\n",
      "Epoch 58/60\n",
      "63/63 [==============================] - 14s 216ms/step - loss: 0.0605 - accuracy: 0.9795 - val_loss: 0.0176 - val_accuracy: 0.9960\n",
      "Epoch 59/60\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.0604 - accuracy: 0.9790 - val_loss: 0.0740 - val_accuracy: 0.9705\n",
      "Epoch 60/60\n",
      "63/63 [==============================] - 14s 225ms/step - loss: 0.0667 - accuracy: 0.9815 - val_loss: 0.0307 - val_accuracy: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bc1a7c5518>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Perform Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Prediction: cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\single_prediction\\cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "  \n",
    "print(\"Image Prediction: \" + prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Image 1: dog\n",
      "Prediction for Image 2: cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def predict_image(classifier, image_path):\n",
    "    test_image = image.load_img(image_path, target_size=(64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    result = classifier.predict(test_image)\n",
    "\n",
    "    class_indices = training_set.class_indices\n",
    "\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'dog'\n",
    "    else:\n",
    "        prediction = 'cat'\n",
    "\n",
    "    return prediction\n",
    "\n",
    "image1_path = r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\single_prediction\\cat_or_dog_1.jpg'\n",
    "image2_path = r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\single_prediction\\cat_or_dog_2.jpg'\n",
    "\n",
    "prediction1 = predict_image(cnn, image1_path)\n",
    "prediction2 = predict_image(cnn, image2_path)\n",
    "\n",
    "print(f\"Prediction for Image 1: {prediction1}\")\n",
    "print(f\"Prediction for Image 2: {prediction2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
