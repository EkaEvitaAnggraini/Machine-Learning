{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Description\n",
=======
    "### Description\n",
>>>>>>> origin/main
    "In this practical exercise, we will create a simple CNN classification model for the case of cat and dog images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Import Library"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pre Processing\n",
    "\n",
    "In this stage, we will perform some image manipulations on the images used. The manipulations include pixel value normalization, tilt correction, enlargement (zoom), and flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1. Training Data Pre Processing**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> origin/main
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
<<<<<<< HEAD
    "training_set = train_datagen.flow_from_directory(r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\test_set',\n",
=======
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
>>>>>>> origin/main
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.2. Testing Data Pre Processing**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\test_set',\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
>>>>>>> origin/main
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Build CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.1. - CNN Model Initialization**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.2. - Add 1st Convolutional Layer**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.3. - Add 1st Pooling Layer**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.4. - Add 2nd Convolutional Layer and Pooling Layer**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.5. - Flattening**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.6. - Add Fully Connected Layer (Input and 1st Hidden Layer)**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.7. Add Fully Connected Layer (The Output Layer)**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.8. - Compile CNN Model**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": null,
>>>>>>> origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "63/63 [==============================] - 14s 209ms/step - loss: 0.6908 - accuracy: 0.5420 - val_loss: 0.6654 - val_accuracy: 0.6385\n",
      "Epoch 2/60\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.6635 - accuracy: 0.5975 - val_loss: 0.6523 - val_accuracy: 0.5870\n",
      "Epoch 3/60\n",
      "63/63 [==============================] - 14s 231ms/step - loss: 0.6212 - accuracy: 0.6630 - val_loss: 0.5864 - val_accuracy: 0.6780\n",
      "Epoch 4/60\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.6084 - accuracy: 0.6655 - val_loss: 0.5726 - val_accuracy: 0.7035\n",
      "Epoch 5/60\n",
      "63/63 [==============================] - 14s 218ms/step - loss: 0.5704 - accuracy: 0.7065 - val_loss: 0.5191 - val_accuracy: 0.7430\n",
      "Epoch 6/60\n",
      "63/63 [==============================] - 13s 211ms/step - loss: 0.5367 - accuracy: 0.7375 - val_loss: 0.5617 - val_accuracy: 0.7175\n",
      "Epoch 7/60\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.5242 - accuracy: 0.7370 - val_loss: 0.4662 - val_accuracy: 0.7825\n",
      "Epoch 8/60\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.5266 - accuracy: 0.7360 - val_loss: 0.4891 - val_accuracy: 0.7595\n",
      "Epoch 9/60\n",
      "63/63 [==============================] - 18s 292ms/step - loss: 0.5055 - accuracy: 0.7560 - val_loss: 0.4678 - val_accuracy: 0.7785\n",
      "Epoch 10/60\n",
      "63/63 [==============================] - 17s 268ms/step - loss: 0.4928 - accuracy: 0.7600 - val_loss: 0.4215 - val_accuracy: 0.8155\n",
      "Epoch 11/60\n",
      "63/63 [==============================] - 14s 220ms/step - loss: 0.4704 - accuracy: 0.7785 - val_loss: 0.5504 - val_accuracy: 0.7210\n",
      "Epoch 12/60\n",
      "63/63 [==============================] - 14s 228ms/step - loss: 0.4503 - accuracy: 0.7885 - val_loss: 0.4327 - val_accuracy: 0.7940\n",
      "Epoch 13/60\n",
      "63/63 [==============================] - 14s 219ms/step - loss: 0.4520 - accuracy: 0.7800 - val_loss: 0.4363 - val_accuracy: 0.7790\n",
      "Epoch 14/60\n",
      "63/63 [==============================] - 14s 215ms/step - loss: 0.4431 - accuracy: 0.7925 - val_loss: 0.3528 - val_accuracy: 0.8430\n",
      "Epoch 15/60\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.4231 - accuracy: 0.8015 - val_loss: 0.3673 - val_accuracy: 0.8255\n",
      "Epoch 16/60\n",
      "63/63 [==============================] - 18s 280ms/step - loss: 0.4152 - accuracy: 0.8080 - val_loss: 0.3336 - val_accuracy: 0.8695\n",
      "Epoch 17/60\n",
      "63/63 [==============================] - 14s 224ms/step - loss: 0.4052 - accuracy: 0.8155 - val_loss: 0.3209 - val_accuracy: 0.8620\n",
      "Epoch 18/60\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.3705 - accuracy: 0.8350 - val_loss: 0.3464 - val_accuracy: 0.8420\n",
      "Epoch 19/60\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.3705 - accuracy: 0.8260 - val_loss: 0.3125 - val_accuracy: 0.8635\n",
      "Epoch 20/60\n",
      "63/63 [==============================] - 13s 213ms/step - loss: 0.3506 - accuracy: 0.8520 - val_loss: 0.2748 - val_accuracy: 0.8840\n",
      "Epoch 21/60\n",
      "63/63 [==============================] - 13s 214ms/step - loss: 0.3365 - accuracy: 0.8560 - val_loss: 0.2787 - val_accuracy: 0.8735\n",
      "Epoch 22/60\n",
      "63/63 [==============================] - 14s 228ms/step - loss: 0.3342 - accuracy: 0.8550 - val_loss: 0.2546 - val_accuracy: 0.8975\n",
      "Epoch 23/60\n",
      "63/63 [==============================] - 13s 211ms/step - loss: 0.3203 - accuracy: 0.8635 - val_loss: 0.2302 - val_accuracy: 0.9115\n",
      "Epoch 24/60\n",
      "63/63 [==============================] - 13s 210ms/step - loss: 0.3169 - accuracy: 0.8695 - val_loss: 0.2545 - val_accuracy: 0.8905\n",
      "Epoch 25/60\n",
      "63/63 [==============================] - 13s 208ms/step - loss: 0.3045 - accuracy: 0.8705 - val_loss: 0.2243 - val_accuracy: 0.9055\n",
      "Epoch 26/60\n",
      "63/63 [==============================] - 13s 210ms/step - loss: 0.2724 - accuracy: 0.8875 - val_loss: 0.2020 - val_accuracy: 0.9215\n",
      "Epoch 27/60\n",
      "63/63 [==============================] - 14s 231ms/step - loss: 0.2733 - accuracy: 0.8845 - val_loss: 0.2022 - val_accuracy: 0.9165\n",
      "Epoch 28/60\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.2528 - accuracy: 0.8920 - val_loss: 0.2353 - val_accuracy: 0.8990\n",
      "Epoch 29/60\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.2324 - accuracy: 0.9120 - val_loss: 0.1514 - val_accuracy: 0.9410\n",
      "Epoch 30/60\n",
      "63/63 [==============================] - 14s 226ms/step - loss: 0.2254 - accuracy: 0.9100 - val_loss: 0.1356 - val_accuracy: 0.9540\n",
      "Epoch 31/60\n",
      "63/63 [==============================] - 14s 218ms/step - loss: 0.2121 - accuracy: 0.9165 - val_loss: 0.1455 - val_accuracy: 0.9390\n",
      "Epoch 32/60\n",
      "63/63 [==============================] - 13s 209ms/step - loss: 0.2117 - accuracy: 0.9125 - val_loss: 0.1345 - val_accuracy: 0.9505\n",
      "Epoch 33/60\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.2090 - accuracy: 0.9140 - val_loss: 0.1595 - val_accuracy: 0.9370\n",
      "Epoch 34/60\n",
      "63/63 [==============================] - 13s 207ms/step - loss: 0.1954 - accuracy: 0.9210 - val_loss: 0.1244 - val_accuracy: 0.9585\n",
      "Epoch 35/60\n",
      "63/63 [==============================] - 13s 206ms/step - loss: 0.1740 - accuracy: 0.9340 - val_loss: 0.0884 - val_accuracy: 0.9705\n",
      "Epoch 36/60\n",
      "63/63 [==============================] - 14s 223ms/step - loss: 0.1601 - accuracy: 0.9340 - val_loss: 0.0943 - val_accuracy: 0.9675\n",
      "Epoch 37/60\n",
      "63/63 [==============================] - 17s 272ms/step - loss: 0.1533 - accuracy: 0.9425 - val_loss: 0.1335 - val_accuracy: 0.9480\n",
      "Epoch 38/60\n",
      "63/63 [==============================] - 16s 259ms/step - loss: 0.1542 - accuracy: 0.9415 - val_loss: 0.0915 - val_accuracy: 0.9700\n",
      "Epoch 39/60\n",
      "63/63 [==============================] - 17s 276ms/step - loss: 0.1400 - accuracy: 0.9465 - val_loss: 0.1193 - val_accuracy: 0.9525\n",
      "Epoch 40/60\n",
      "63/63 [==============================] - 16s 252ms/step - loss: 0.1361 - accuracy: 0.9455 - val_loss: 0.0708 - val_accuracy: 0.9755\n",
      "Epoch 41/60\n",
      "63/63 [==============================] - 16s 257ms/step - loss: 0.1192 - accuracy: 0.9500 - val_loss: 0.0798 - val_accuracy: 0.9680\n",
      "Epoch 42/60\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.1208 - accuracy: 0.9490 - val_loss: 0.0943 - val_accuracy: 0.9660\n",
      "Epoch 43/60\n",
      "63/63 [==============================] - 16s 252ms/step - loss: 0.1214 - accuracy: 0.9550 - val_loss: 0.0621 - val_accuracy: 0.9830\n",
      "Epoch 44/60\n",
      "63/63 [==============================] - 16s 256ms/step - loss: 0.1135 - accuracy: 0.9530 - val_loss: 0.0521 - val_accuracy: 0.9845\n",
      "Epoch 45/60\n",
      "63/63 [==============================] - 22s 353ms/step - loss: 0.0987 - accuracy: 0.9620 - val_loss: 0.0515 - val_accuracy: 0.9840\n",
      "Epoch 46/60\n",
      "63/63 [==============================] - 18s 283ms/step - loss: 0.1039 - accuracy: 0.9630 - val_loss: 0.0403 - val_accuracy: 0.9915\n",
      "Epoch 47/60\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1047 - accuracy: 0.9625 - val_loss: 0.0748 - val_accuracy: 0.9700\n",
      "Epoch 48/60\n",
      "63/63 [==============================] - 18s 287ms/step - loss: 0.0794 - accuracy: 0.9730 - val_loss: 0.0288 - val_accuracy: 0.9955\n",
      "Epoch 49/60\n",
      "63/63 [==============================] - 18s 279ms/step - loss: 0.0919 - accuracy: 0.9670 - val_loss: 0.0302 - val_accuracy: 0.9935\n",
      "Epoch 50/60\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.0844 - accuracy: 0.9725 - val_loss: 0.0393 - val_accuracy: 0.9860\n",
      "Epoch 51/60\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0813 - accuracy: 0.9680 - val_loss: 0.0261 - val_accuracy: 0.9940\n",
      "Epoch 52/60\n",
      "63/63 [==============================] - 14s 216ms/step - loss: 0.0763 - accuracy: 0.9720 - val_loss: 0.0279 - val_accuracy: 0.9925\n",
      "Epoch 53/60\n",
      "63/63 [==============================] - 14s 221ms/step - loss: 0.0631 - accuracy: 0.9780 - val_loss: 0.0259 - val_accuracy: 0.9945\n",
      "Epoch 54/60\n",
      "63/63 [==============================] - 14s 225ms/step - loss: 0.0885 - accuracy: 0.9720 - val_loss: 0.0256 - val_accuracy: 0.9945\n",
      "Epoch 55/60\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.0779 - accuracy: 0.9735 - val_loss: 0.0414 - val_accuracy: 0.9850\n",
      "Epoch 56/60\n",
      "63/63 [==============================] - 14s 221ms/step - loss: 0.0714 - accuracy: 0.9760 - val_loss: 0.0191 - val_accuracy: 0.9970\n",
      "Epoch 57/60\n",
      "63/63 [==============================] - 14s 220ms/step - loss: 0.0648 - accuracy: 0.9825 - val_loss: 0.0460 - val_accuracy: 0.9860\n",
      "Epoch 58/60\n",
      "63/63 [==============================] - 14s 226ms/step - loss: 0.0682 - accuracy: 0.9730 - val_loss: 0.0540 - val_accuracy: 0.9825\n",
      "Epoch 59/60\n",
      "63/63 [==============================] - 14s 226ms/step - loss: 0.0462 - accuracy: 0.9835 - val_loss: 0.0206 - val_accuracy: 0.9945\n",
      "Epoch 60/60\n",
      "63/63 [==============================] - 14s 216ms/step - loss: 0.0628 - accuracy: 0.9770 - val_loss: 0.0261 - val_accuracy: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bc1a1aaa58>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set, validation_data=test_set, epochs=60)"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Perform Single Prediction"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Prediction: cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\single_prediction\\cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "  \n",
    "print(\"Image Prediction: \" + prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Image 1: dog\n",
      "Prediction for Image 2: cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def predict_image(classifier, image_path):\n",
    "    test_image = image.load_img(image_path, target_size=(64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    result = classifier.predict(test_image)\n",
    "\n",
    "    class_indices = training_set.class_indices\n",
    "\n",
    "    if result[0][0] == 1:\n",
    "        prediction = 'dog'\n",
    "    else:\n",
    "        prediction = 'cat'\n",
    "\n",
    "    return prediction\n",
    "\n",
    "image1_path = r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\single_prediction\\cat_or_dog_1.jpg'\n",
    "image2_path = r'D:\\Github Folder\\Machine-Learning\\data\\dataset-11\\single_prediction\\cat_or_dog_2.jpg'\n",
    "\n",
    "prediction1 = predict_image(cnn, image1_path)\n",
    "prediction2 = predict_image(cnn, image2_path)\n",
    "\n",
    "print(f\"Prediction for Image 1: {prediction1}\")\n",
    "print(f\"Prediction for Image 2: {prediction2}\")"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
>>>>>>> origin/main
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
