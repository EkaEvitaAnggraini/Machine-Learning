{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "hiITCUjm991m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "nGwjpfzd-AXV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ1ricGW-FlG",
        "outputId": "041dad8f-a9a7-430a-8a9d-3985ebf5e5e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtlsTY8G-JT1",
        "outputId": "6250ad02-8389-4336-e9ed-11259136c214"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He_gMiM9-LWo",
        "outputId": "c5891d3b-bb3b-4724-c90d-f7e017543da9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZbiJIwc-N7s",
        "outputId": "d13302b9-c90c-4771-e333-755275f2583a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "ADcqQs5X-Wsb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACOSe_l2-cW9",
        "outputId": "8d612026-4451-45df-a2fe-f90d0cccef68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "Vq4JhQ4Y-gbP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK3oqw3N-jHS",
        "outputId": "d8c5e726-05ac-46b0-a430-0ab7aaf3e56b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_from_ids = lambda ids: tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "zEtO-RhN-pX-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3xUF3Qe-s1A",
        "outputId": "6c98d01c-cbcc-47f4-9122-c63d8ad0c92d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "hfBZD4ij-xDz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHlGVg8a-0LX",
        "outputId": "bec8f476-aa37-48cc-f43e-698b63fa9cf2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "xIOuu4Fp-6dl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auFsHE-f--HI",
        "outputId": "83ae8bdf-f2e6-4ec1-9054-c97385a13a38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "OPDB6eRqL6nq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwt2TrZGMDGk",
        "outputId": "7b0962db-8c8f-4d15-9193-dabe3854ceb5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "mXs4ESiHMFZ4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIqWZ9kkMItC",
        "outputId": "bf8a02b9-da38-490a-d8fe-1bd82e272479"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8LBsWQ5N0W7",
        "outputId": "06baeb09-f521-4ff7-e034-9d7db334db04"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "8HcH_DEAN3SA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "X6yVx31rN5rp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "ZAkhfBw6N78A"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV-XweyBN-Rf",
        "outputId": "e4562249-a5a0-4620-d737-6a77be672123"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUPc-4ZHOApM",
        "outputId": "7c1f8f7b-b917-41a7-e10c-39f13804a7d7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "Y99Me86GOEQB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs3zRkFcOJwu",
        "outputId": "8a80d221-9a67-48a0-b99c-4374939cf060"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([30, 14, 49, 28, 58, 47, 37, 47, 34, 28, 28, 33,  5, 59, 16, 23,  1,\n",
              "       61, 24, 43,  0, 53, 23,  7, 20,  8, 48, 28, 38, 51, 50, 17, 51, 33,\n",
              "       23, 41, 39, 56, 57,  7, 29, 54, 56, 46, 16, 62, 32, 43, 37, 62, 32,\n",
              "       14, 10, 35, 25, 30, 46,  4,  2,  7, 12,  4, 20, 28, 39, 12, 49,  7,\n",
              "       38, 31, 46,  2, 20, 51, 27,  7, 65, 35, 64, 55, 49, 26, 61, 60, 47,\n",
              "       41,  0, 44, 31, 43, 60,  1, 53,  1, 27, 42, 27, 13, 26, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnLYdb0GOM8P",
        "outputId": "ed05dad4-4774-48a3-ee3f-965fb89e76ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b're I move,\\nWhat my tongue speaks my right drawn sword may prove.\\n\\nTHOMAS MOWBRAY:\\nLet not my cold wo'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'QAjOshXhUOOT&tCJ\\nvKd[UNK]nJ,G-iOYlkDlTJbZqr,PoqgCwSdXwSA3VLQg$ ,;$GOZ;j,YRg GlN,zVypjMvuhb[UNK]eRdu\\nn\\nNcN?MO'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "MIkgWEc7OSmI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_wk-g2HOZjW",
        "outputId": "c377a826-9e40-4779-cb68-19e51319b132"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189736, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BwFn9F_ObBd",
        "outputId": "1cd69649-6e69-48d8-c85b-5467b1ef7b1d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.005356"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "gBFGZd7ROdSv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "VwkcDa7UOgkH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "5pxLe1XlOjGz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqk0UHVcOqFF",
        "outputId": "e609d810-a621-4071-ff0e-cbcfa0b23f1b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 57ms/step - loss: 2.7390\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 2.0027\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 14s 57ms/step - loss: 1.7192\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 13s 56ms/step - loss: 1.5548\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.4545\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 13s 54ms/step - loss: 1.3847\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3312\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2850\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.2442\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 14s 60ms/step - loss: 1.2039\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1645\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1233\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.0803\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.0344\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.9860\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9357\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8836\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8314\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 0.7810\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 0.7330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "9peQrOFZOvBd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "eLvOSg8FOv06"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3S6-qKGO0Aw",
        "outputId": "c3150499-c143-42fb-a77c-03acc74c5233"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I have but smooth;\n",
            "For one being confound, even as they set of.\n",
            "I tell thee, I beseech thee, fellow,\n",
            "Post thou remains, and Richard killer than\n",
            "Upon the rocks in him Titids:\n",
            "tread, one of their blood, whose injury\n",
            "The axe use to kiss it: else, his gracious sea,\n",
            "Wrong hate, do stand, and will forget him put\n",
            "one, My gravics Lord Anno, Lucentio?\n",
            "\n",
            "ROMEO:\n",
            "Stay to, thou shouldst keep his wreck; where stands thus mind;\n",
            "You shall ome other dreams.\n",
            "\n",
            "KING RICHARD II:\n",
            "We are alike to be agredian:\n",
            "This is my father, for she hath some paut,\n",
            "In human hands, for love I beg,\n",
            "Obulia sovereign,' to their cryoting in\n",
            "His man betain'd the gracious use,\n",
            "Fell murder death most unruly, whereon I shall over-look and\n",
            "privanely.\n",
            "\n",
            "CORIOLANUS:\n",
            "First, for the lists,\n",
            "Wherewite does extirent distonour live;\n",
            "Whose salt is hate, take it with death is made,\n",
            "Head not my queen, unwilting belongs,\n",
            "Myself tongues to be myself.\n",
            "\n",
            "MOMNASTA:\n",
            "If then be gone,--\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Why have you now, not to the late.\n",
            "\n",
            "First Servin \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4016759395599365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "O-YOdrCnPDY-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSG0aZSZO11u",
        "outputId": "21517038-d905-4ff8-a44a-81fb7c793282"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nI have at least like O nine young consul:\\nStill not my curse and make thine eyes,\\nI would I sleep in quiet. Show thou art extreme pluce\\nEre he early noble that must be question,\\nAnd bear their love advised the king:\\nFor one poor grain or two?\\nI will not gladly in hungers' parts\\nThat have be set down towards Livilio.\\n\\nWARWICK:\\nAy, that's the name; may it please you, after I.\\nI can no longer breath his beard\\nAnd never the aduse me but to death.\\n\\nHENRY BOLINGBROKE:\\nI know thou wilt, let them not hell.\\n\\nAUTOLYCUS:\\nHere's the mistake me inquired man:\\nTherefore follow me, it did fires of it.\\n\\nGentleman:\\nMy lord to hold you from the torch?\\nYou have and love me, travelling, if he be long.\\nAs he fault banish with my rate's unknated,\\nI should push on the oracle would,\\nFor the report of her kissen hand,\\nBut set his prenzies could speak not on\\nThe law come on; it is no got to be.\\n\\nPETRUCHIO:\\nGo by the holy churchyard their promises.\\n\\nFirst Hupan,\\nAnd I do so in me and hearts:\\nThough the Earl of W\"\n",
            " b\"ROMEO:\\nThe more I may with fire, a rettle ruth,\\nAnd thus I part thee a tone, and with their loves to clame\\nTo swept his nature, may I call thee, I\\nnot scake thee, at the sty deposed?\\n\\nKING RICHARD III:\\nStanley, I will. Plother, I say, and a heaven so well.\\n\\nAUTOLYCUS:\\nTrial. Did thee good time, the devil's day.\\n\\nDUKE OF YORK:\\nGo to the gamor of Antium? I'll woo already:\\n'Tis so to enter into the time:\\nAnd in the duke is rank the same young Edward,\\nYour daughters' loves and all by here both\\nDespite of work, you rather, though my pace\\nIf heaven blinds; it has not digerted labour,\\nCome I have with the kingdom of fortune seal:\\nNorthampt not her traitor: doth the high speecht\\nhad need of me upon our name: of Warwick,\\nNeat it bethereth, not region: and their fearful honour,\\nProceed that stones more fair shall; quite for his sake.\\n\\nQUEEN:\\nCanstat-san to that father?\\nLet us task-for me or wife so false,\\nAnd slew him like a bow: the sin hath done\\nNot where thou liest, that thou can auther over all as\"\n",
            " b\"ROMEO:\\nI saw him water him to treason.\\nDost thou not, I, fear'st thou to Friar Percamp\\nSenators: he, my gracious jurk-hat,\\nnieed that I shall come woe. O gracious\\nAre more than an unstanch with other: I must do\\nwith tears and days thou lusty the ground.\\nThe law each other reconcile\\nThat die this dark man talk of. Set out thy state,\\nEre such a lady's rage, where scorn'd in Padua;\\nAnd then, to do but sword a life: no hares,\\nnot his own behalf; my tender nearth's flesh and sight\\nThat valiant in this breath I should,\\nAnd as I play the good is enemy's greatest, here it is\\nAs fresh as mill-over heart a fearful flight\\nFrom toward three manks, to say to deeas Frottling.\\nAnd how could he shall lose his head;\\nAnd wherefore camest thou, it should seek to take up mine oratory,\\nOr else yone sir; my wisdom wailing,\\nDown, knowing as it was, my lord, and shill tell thee no shall of men\\nThan Romeo cas gone dears, I take my\\nlects to doth him seek your grace,\\nBut for my life must think it, most obediee.\\n\\nPARIS\"\n",
            " b\"ROMEO:\\nI will thereto depart, he goes to cherish;\\nAnd being in her earth, and noise ignorance,\\nTo men and I can could go well when I saw my love\\nTo broog her that the odds findly double with not\\nAs I incern by warshing fring it thine,\\nOne party tentlemen; it should be short,\\nShall I not hear 'twixt pity of that pawn;\\nTeach very treasure known to meet you are:\\nWhence are you seal that reason doubt not how it was a-pedforce,\\nOr ill--shall go along, this favour with been\\nAntinoned, all fully of their love\\nA mean to combut them their sentiness.\\n\\nPRINCE:\\nCome thou, and mistress which shall not have talk'd\\nUnto the false light gloacy thither of mine:\\nYour story hath golden vain so noble;\\nMay this, in any gentlewoman.\\n\\nNurse:\\nIs the gate friar?\\n\\nDUKE VINCENTIO:\\nLet him that hands did safe.\\n\\nCLARENCE:\\nI will require you touch the idle honour of\\nthe man that trouble this man's life\\nAnd how cursed in mine arms, and most contruach,\\nIn that into their own life scandal.\\n\\nNurse:\\nMade God make it, you may \"\n",
            " b\"ROMEO:\\nHere in this prop the world, are afterworn.\\nJack, roable son, that lies i' the oracle\\nGive me to torme that love to Rame's excuse.\\n\\nLUCENTIO:\\nWell, then, there's no remedy:\\nIf I should say, 'I would be in the mind,\\nBut choke it in thy tarkeris selfers\\nClaudio disloy'd her my discorts me a full till\\nAnother leader, though I with dives in blind,\\nI do remain affect their own days on the square\\nAnd frame him now to be a little babe:\\nYour mean marvellous letters from my speed!\\nAmaze we may: then will I do, and Rome\\nShould show thee: thou art to be most conclude,\\nTo make it England, and for reverse reason\\nthe prefane a thing importunney, throwers,\\nAnd all the queen is left of this:\\nyou are we self for consiliant flatter these\\nState his own ears: take themselves, their ancient palace.\\n\\nKING RICHARD II:\\nNorfolk, the match'd hath valiant Selanon;\\nAnd so, my good lord, may methink it:\\nIn Gloucester knee. If hasty said is he\\nsince for this hour, I swear. If you had been\\nThe account of the julict\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.773957252502441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T20B6mo7O4AS",
        "outputId": "ccf4d300-cc0c-436e-935d-59f73d001d36"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7926821939d0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPa1dCMgPNwZ",
        "outputId": "ab50fb4f-714c-4aad-e879-66350a0ea01d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Thou hast far forth feedly; yet, that rogued,\n",
            "I cannot blow you for that newer torture\n",
            "Turtly again\n"
          ]
        }
      ]
    }
  ]
}